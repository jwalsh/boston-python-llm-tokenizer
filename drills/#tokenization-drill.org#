* Tokenization Drills

** What is Tokenization?                                              :drill:
:PROPERTIES:
:ID:       289A0768-35E7-49B6-8497-C7533E6673D7
:END:

Tokenization is the process of breaking down [ text ] into individual words or tokens.

This is a fundamental step in natural language processing (NLP) and is used in a variety of applications, including

- [ text]  analysis,
- [ sentiment ] analysis, and
- [ machine ]  translation. 

** Tokenization Example 1: Simple Sentence                            :drill:
:PROPERTIES:
:ID:       92BC3C5A-5F22-4515-88CD-8259B6287823
:END:
Tokenize the following sentence:
"The quick brown fox jumps over the lazy dog."
*** Answer ***
#+BEGIN_SRC python
import nltk
nltk.download('punkt')
sentence = "The quick brown fox jumps over the lazy dog."
tokens = nltk.word_tokenize(sentence)
print(tokens)
#+END_SRC
** Tokenization Example 2: Sentence with Punctuation                  :drill:
:PROPERTIES:
:ID:       89941444-989B-412D-93A2-2A70D2FDD4E4
:END:
Tokenize the following sentence:
"Hello, world! How are you today?"
*** Answer ***
#+BEGIN_SRC python
import nltk
nltk.download('punkt')
sentence = "Hello, world! How are you today?"
tokens = nltk.word_tokenize(sentence)
print(tokens)
#+END_SRC
** Tokenization Example 3: Paragraph                                  :drill:
:PROPERTIES:
:ID:       311D2F0B-32FF-4AEA-B0A8-83EC61DAEE30
:END:
Tokenize the following paragraph:
"The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky."
*** Answer ***
#+BEGIN_SRC python
import nltk
nltk.download('punkt')
paragraph = "The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky."
tokens = nltk.word_tokenize(paragraph)
print(tokens)
#+END_SRC
** Tokenization Example 4: ID Generation                              :drill:
:PROPERTIES:
:ID:       04720C13-946F-4C96-A5B2-94D5A3A9EB64
:END:
Generate a unique ID for each token in the following sentence:
"The quick brown fox jumps over the lazy dog."
*** Answer ***
#+BEGIN_SRC python
import uuid
sentence = "The quick brown fox jumps over the lazy dog."
tokens = sentence.split()
ids = [str(uuid.uuid4()) for token in tokens]
print(ids)
#+END_SRC
** Tokenization Example 5: Using An Anthropic Model                   :drill:
:PROPERTIES:
:ID:       CCC6F13F-1034-45E0-BE9B-3E6B6CC4D1FA
:END:
Use the Anthropic model to tokenize the following sentence:
"The quick brown fox jumps over the lazy dog."
*** Answer ***
#+BEGIN_SRC python
import anthropic
model = anthropic.Model()
sentence = "The quick brown fox jumps over the lazy dog."
tokens = model.tokenize(sentence)
print(tokens)
#+END_SRC
** Tokenization Example 6: Using Langchain                            :drill:
:PROPERTIES:
:ID:       DBDFA460-E244-4540-94CE-A5C0236A4B9C
:END:
Use Langchain to tokenize the following sentence:
"The quick brown fox jumps over the lazy dog."
*** Answer ***
#+BEGIN_SRC python
import langchain
model = langchain.Model()
sentence = "The quick brown fox jumps over the lazy dog."
tokens = model.tokenize(sentence)
print(tokens)
#+END_SRC
** Tokenization Example 7: Using OpenAI                               :drill:
:PROPERTIES:
:ID:       7DAF26A4-299F-40BE-B7FD-EA2A2F639478
:END:
Use OpenAI to tokenize the following sentence:
"The quick brown fox jumps over the lazy dog."
*** Answer ***
#+BEGIN_SRC python
import openai
model = openai.Model()
sentence = "The quick brown fox jumps over the lazy dog."
tokens = model.tokenize(sentence)
print(tokens)
#+END_SRC
** Tokenization Example 8: Using Hugging Face                         :drill:
:PROPERTIES:
:ID:       C640977F-CDE8-4425-B7C8-410E185E7F3B
:END:
Use Hugging Face to tokenize the following sentence:
"The quick brown fox jumps over the lazy dog."
*** Answer ***
#+BEGIN_SRC python
import transformers
model = transformers.AutoModelForTokenClassification.from_pretrained('bert-base-uncased')
tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')
sentence = "The quick brown fox jumps over the lazy dog."
inputs = tokenizer(sentence, return_tensors='pt')
outputs = model(**inputs)
print(outputs)
#+END_SRC
** Tokenization Example 9: Tokenizing a Text File                     :drill:
:PROPERTIES:
:ID:       9BEA7F4C-3879-45B7-9E06-C3ABA80C5A1C
:END:
Tokenize the text in the following file: example.txt
*** Answer ***
#+BEGIN_SRC python
import nltk
nltk.download('punkt')
with open('example.txt', 'r') as f:
text = f.read()
tokens = nltk.word_tokenize(text)
print(tokens)
#+END_SRC
** Tokenization Example 10: Tokenizing a Web Page                     :drill:
:PROPERTIES:
:ID:       5C687AD9-989F-4E28-A2BA-F3403852568B
:END:
Tokenize the text on the following web page: https://www.example.com
*** Answer ***
#+BEGIN_SRC python
import nltk
nltk.download('punkt')
import requests
from bs4 import BeautifulSoup
url = ''
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
text = soup.get_text()
tokens = nltk.word_tokenize(text)
print(tokens)
#+END_SRC
