* Tokenization Drills

** What is Tokenization?                                              :drill:
:PROPERTIES:
:ID:       8EFEA813-F9A5-4FA7-9C7E-4CF24793646B
:END:

Tokenization is the process of breaking down text into individual words or tokens.

This is a fundamental step in natural language processing (NLP) and is used in a variety of applications, including:

- Text analysis
- Sentiment analysis
- Machine translation 

** Tokenization Example 1: Simple Sentence                            :drill:
:PROPERTIES:
:ID:       2CABE481-2383-4E3E-A246-208458E3DABC
:END:
Tokenize the following sentence:

#+begin_example
"The quick brown fox jumps over the lazy dog."
#+end_example

*** Answer

#+BEGIN_SRC python
import nltk
nltk.download('punkt')
sentence = "The quick brown fox jumps over the lazy dog."
tokens = nltk.word_tokenize(sentence)
print(tokens)
#+END_SRC

** Tokenization Example 2: Sentence with Punctuation                  :drill:
:PROPERTIES:
:ID:       BA0014B1-C096-4559-A789-B2DB971C8FE7
:END:

Tokenize the following sentence:

#+begin_example
"Hello, world! How are you today?"
#+end_example


*** Answer

#+BEGIN_SRC python
import nltk
nltk.download('punkt')
sentence = "Hello, world! How are you today?"
tokens = nltk.word_tokenize(sentence)
print(tokens)
#+END_SRC

** Tokenization Example 3: Paragraph                                  :drill:
:PROPERTIES:
:ID:       BA64D2DE-516E-4D7D-826D-596970EA8190
:END:

Tokenize the following paragraph:

#+begin_example
"The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky."
#+end_example

*** Answer

#+BEGIN_SRC python
import nltk
nltk.download('punkt')
paragraph = "The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky."
tokens = nltk.word_tokenize(paragraph)
print(tokens)
#+END_SRC

** Tokenization Example 4: Using Hugging Face Tokenizer               :drill:
:PROPERTIES:
:ID:       06FCEEDB-85E3-46D9-BF86-92DD35DA3EBF
:END:
Use Hugging Face to tokenize the following sentence:

#+begin_example
"The quick brown fox jumps over the lazy dog."
#+end_example

*** Answer

#+BEGIN_SRC python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
sentence = "The quick brown fox jumps over the lazy dog."
tokens = tokenizer.tokenize(sentence)
print(tokens)
#+END_SRC

** Tokenization Example 5: Tokenizing a Text File                     :drill:
:PROPERTIES:
:ID:       A4ED710D-A665-4478-A2F3-2C908CF301EE
:END:

Tokenize the text in the following file: ~example.txt~

*** Answer

#+BEGIN_SRC python
import nltk
nltk.download('punkt')
with open('example.txt', 'r') as f:
    text = f.read()
tokens = nltk.word_tokenize(text)
print(tokens)
#+END_SRC

** Tokenization Example 6: Tokenizing with Regular Expressions        :drill:
:PROPERTIES:
:ID:       5809923A-21AC-4663-9C9D-A059A6107CF7
:END:

Use regular expressions to tokenize the following text:

#+begin_example
"Hello world! This is a test, with some numbers (12, 34, and 56) included."
#+end_example

*** Answer

#+BEGIN_SRC python
import re
text = "Hello world! This is a test, with some numbers (12, 34, and 56) included."
tokens = re.findall(r'\b\w+\b|\S', text)
print(tokens)
#+END_SRC

** Tokenization Example 7: Subword Tokenization                       :drill:
:PROPERTIES:
:ID:       B77D1BA1-637E-4CCE-BB84-02A13E686FE9
:END:

Use a subword tokenization method (like BPE) to tokenize the following word:

#+begin_example
"unconditionally"
#+end_example

*** Answer

#+BEGIN_SRC python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")  # GPT-2 uses BPE tokenization
word = "unconditionally"
tokens = tokenizer.tokenize(word)
print(tokens)
#+END_SRC

** Tokenization Example 8: Multilingual Tokenization                  :drill:
:PROPERTIES:
:ID:       9963C49C-0AE0-4356-ADE2-54B98F96A3EA
:END:
Tokenize the following multilingual text:

#+begin_example
"Hello world! Bonjour le monde! Hola mundo! „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïåÔºÅ"
#+end_example

*** Answer

#+BEGIN_SRC python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
text = "Hello world! Bonjour le monde! Hola mundo! „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïåÔºÅ"
tokens = tokenizer.tokenize(text)
print(tokens)
#+END_SRC

** Tokenization Example 9: Handling Special Characters                :drill:
:PROPERTIES:
:ID:       B2268BE3-CDDF-458B-83D8-B3BE0BE437A4
:END:

Tokenize the following text with special characters and emojis:

#+begin_example
"Let's meet @ the caf√©! üòä #tokenization"
#+end_example

*** Answer

#+BEGIN_SRC python
import nltk
from nltk.tokenize import TweetTokenizer
nltk.download('punkt')
text = "Let's meet @ the caf√©! üòä #tokenization"
tokenizer = TweetTokenizer()
tokens = tokenizer.tokenize(text)
print(tokens)
#+END_SRC

** Tokenization Example 10: Custom Tokenization                       :drill:
:PROPERTIES:
:ID:       0C742424-ED26-42DC-B2EC-976852BB3EE0
:END:

Create a custom tokenizer that splits on spaces but keeps punctuation attached to words:

#+begin_example
"Hello, world! This is a test."
#+end_example

*** Answer

#+BEGIN_SRC python
import re

def custom_tokenize(text):
    return re.findall(r"\S+", text)

text = "Hello, world! This is a test."
tokens = custom_tokenize(text)
print(tokens)
#+END_SRC

#+RESULTS:
: None
