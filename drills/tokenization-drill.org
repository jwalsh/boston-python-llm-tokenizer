* Tokenization Drills

** What is Tokenization?                                              :drill:

Tokenization is [ the process of breaking down text into individual words or
tokens. ]

*** Answer

This is a fundamental step in natural language processing (NLP) and is used in a variety of applications, including:

- Text analysis
- Sentiment analysis
- Machine translation 

** Tokenization Example 1: Simple Sentence                            :drill:

Tokenize the following sentence:

#+begin_example
"The quick brown fox jumps over the lazy dog."
#+end_example

*** Answer

#+BEGIN_SRC python
import nltk
nltk.download('punkt')
sentence = "The quick brown fox jumps over the lazy dog."
tokens = nltk.word_tokenize(sentence)
print(tokens)
#+END_SRC

** Tokenization Example 2: Sentence with Punctuation                  :drill:

Tokenize the following sentence:

#+begin_example
"Hello, world! How are you today?"
#+end_example


*** Answer

#+BEGIN_SRC python
import nltk
nltk.download('punkt')
sentence = "Hello, world! How are you today?"
tokens = nltk.word_tokenize(sentence)
print(tokens)
#+END_SRC

** Tokenization Example 3: Paragraph                                  :drill:

Tokenize the following paragraph:

#+begin_example
"The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky."
#+end_example

*** Answer

#+BEGIN_SRC python
import nltk
nltk.download('punkt')
paragraph = "The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky."
tokens = nltk.word_tokenize(paragraph)
print(tokens)
#+END_SRC

** Tokenization Example 4: Using Hugging Face Tokenizer               :drill:

Use Hugging Face to tokenize the following sentence:

#+begin_example
"The quick brown fox jumps over the lazy dog."
#+end_example

*** Answer

#+BEGIN_SRC python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
sentence = "The quick brown fox jumps over the lazy dog."
tokens = tokenizer.tokenize(sentence)
print(tokens)
#+END_SRC

** Tokenization Example 5: Tokenizing a Text File                     :drill:

Tokenize the text in the following file: ~example.txt~

*** Answer

#+BEGIN_SRC python
import nltk
nltk.download('punkt')
with open('example.txt', 'r') as f:
    text = f.read()
tokens = nltk.word_tokenize(text)
print(tokens)
#+END_SRC

** Tokenization Example 6: Tokenizing with Regular Expressions        :drill:

Use regular expressions to tokenize the following text:

#+begin_example
"Hello world! This is a test, with some numbers (12, 34, and 56) included."
#+end_example

*** Answer

#+BEGIN_SRC python
import re
text = "Hello world! This is a test, with some numbers (12, 34, and 56) included."
tokens = re.findall(r'\b\w+\b|\S', text)
print(tokens)
#+END_SRC

** Tokenization Example 7: Subword Tokenization                       :drill:

Use a subword tokenization method (like BPE) to tokenize the following word:

#+begin_example
"unconditionally"
#+end_example

*** Answer

#+BEGIN_SRC python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")  # GPT-2 uses BPE tokenization
word = "unconditionally"
tokens = tokenizer.tokenize(word)
print(tokens)
#+END_SRC

** Tokenization Example 8: Multilingual Tokenization                  :drill:

Tokenize the following multilingual text:

#+begin_example
"Hello world! Bonjour le monde! Hola mundo! „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïåÔºÅ"
#+end_example

*** Answer

#+BEGIN_SRC python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
text = "Hello world! Bonjour le monde! Hola mundo! „Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïåÔºÅ"
tokens = tokenizer.tokenize(text)
print(tokens)
#+END_SRC

** Tokenization Example 9: Handling Special Characters                :drill:

Tokenize the following text with special characters and emojis:

#+begin_example
"Let's meet @ the caf√©! üòä #tokenization"
#+end_example

*** Answer

#+BEGIN_SRC python
import nltk
from nltk.tokenize import TweetTokenizer
nltk.download('punkt')
text = "Let's meet @ the caf√©! üòä #tokenization"
tokenizer = TweetTokenizer()
tokens = tokenizer.tokenize(text)
print(tokens)
#+END_SRC

** Tokenization Example 10: Custom Tokenization                       :drill:

Create a custom tokenizer that splits on spaces but keeps punctuation attached to words:

#+begin_example
"Hello, world! This is a test."
#+end_example

*** Answer

#+BEGIN_SRC python
import re

def custom_tokenize(text):
    return re.findall(r"\S+", text)

text = "Hello, world! This is a test."
tokens = custom_tokenize(text)
print(tokens)
#+END_SRC

#+RESULTS:
: None
